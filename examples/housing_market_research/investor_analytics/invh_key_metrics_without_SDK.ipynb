{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Welcome to the Lab ðŸ¥¼ðŸ§ª</h1>\n",
    "</center>\n",
    "\n",
    "## Tracking Invitation Homes Quarterly Activity From the Properties V2 Endpoint\n",
    "\n",
    "In this notebook, we will analyze Invitation Homes 2024 quarterly activity in the US across in four key metrics\n",
    "- Acquisitions\n",
    "- Rental Listings\n",
    "- Rent Rate\n",
    "- Inventory\n",
    "\n",
    "The notebook is broken up into the following sections:\n",
    "1. Import required packages and setup the Parcl Labs API key and API headers\n",
    "2. Leverage the V2 Prop Endpoint for the Point in Time Metrics (Aquisitions, Rental Listings and Rent Rate)\n",
    "3. Leverage both the V1 Prop Endpoint for the Quarterly Inventory\n",
    "\n",
    "**Reminders:**\n",
    "\n",
    "- You can get your Parcl Labs API key [here](https://dashboard.parcllabs.com/signup) to follow along.\n",
    "\n",
    "- To run this immediately, you can use Google Colab. Remember, you must set your `PARCL_LABS_API_KEY`. Run in Colab --> [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ParclLabs/parcllabs-cookbook/blob/main/examples/housing_market_research/investor_analytics/invh_key_metrics_without_SDK.ipynb)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "- To run this notebook at scale and download data for multiple markets and endpoints, you will need to upgrade your Parcl Labs API account from free to starter to get additional credits. You can easily upgrade at any time by visiting your [Parcl Labs dashboard](https://dashboard.parcllabs.com/login), clicking \"Upgrade Now\" ($99, no commitment). This will unlock more credits immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import required packages and setup the Parcl Labs API key and API headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, install and/or upgrade to the latest verison of the Parcl Labs Python library\n",
    "%pip install --upgrade parcllabs nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import concurrent.futures\n",
    "from parcllabs import ParclLabsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('PARCL_LABS_API_KEY')\n",
    "url = \"https://api.parcllabs.com/v2/property_search\"\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"Authorization\": api_key\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Leverage the V2 Prop Endpoint for the Point in Time Metrics (Aquisitions, Rental Listings and Rent Rate)\n",
    "\n",
    "Since all of these metrics will look at data that is grouped quarterly, we can do this most efficiently by pulling all 2024 Activity for IH in one query (~24000 credits) and then analyze the resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the query payload\n",
    "payload = {\n",
    "    \"parcl_ids\": [5826765],  # National Market\n",
    "    \"property_filters\": {\n",
    "        \"property_types\": [\"SINGLE_FAMILY\"],\n",
    "        \"include_property_details\": True\n",
    "    },\n",
    "    \"event_filters\": {\n",
    "        \"event_names\": [\"SOLD\", \"SOLD_INTER_PORTFOLIO_TRANSFER\", \"RENTAL_PRICE_CHANGE\", \"LISTED_RENT\"],\n",
    "        \"min_event_date\": \"2024-01-01\",\n",
    "        \"max_event_date\": \"2024-12-31\"\n",
    "    },\n",
    "    \"owner_filters\": { \n",
    "        \"owner_name\": [\"INVITATION_HOMES\"] \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch a page of data\n",
    "def fetch_page(offset, limit=10000):\n",
    "    page_url = f\"{url}?limit={limit}&offset={offset}\"\n",
    "    try:\n",
    "        response = requests.post(page_url, json=payload, headers=headers)\n",
    "        response.raise_for_status()  # Raise exception for HTTP errors\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract events with their property IDs\n",
    "        events_data = []\n",
    "        for prop in data.get('data', []):\n",
    "            property_id = prop.get('parcl_property_id')\n",
    "            for event in prop.get('events', []):\n",
    "                events_data.append({\"parcl_property_id\": property_id, **event})\n",
    "                \n",
    "        return events_data, len(data.get('data', []))\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching page at offset {offset}: {e}\")\n",
    "        return [], 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first page and metadata\n",
    "first_page_events, first_page_props = fetch_page(0)\n",
    "all_events = first_page_events.copy()\n",
    "\n",
    "# Get total properties count for pagination\n",
    "meta_resp = requests.post(f\"{url}?limit=1\", json=payload, headers=headers).json()\n",
    "total = meta_resp.get('metadata', {}).get('results', {}).get('total_available', 0)\n",
    "total_pages = (total + 10000 - 1) // 10000  # Ceiling division\n",
    "\n",
    "print(f\"Found {total} properties, fetched page 1 with {first_page_props} properties and {len(first_page_events)} events\")\n",
    "print(f\"Fetching {total_pages-1} remaining pages in parallel\")\n",
    "\n",
    "# Prepare offsets for remaining pages\n",
    "offsets = [i * 10000 for i in range(1, total_pages)]\n",
    "\n",
    "# Use parallel processing for remaining pages\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=min(10, len(offsets))) as executor:\n",
    "    # Submit all page requests\n",
    "    futures = {executor.submit(fetch_page, offset): offset for offset in offsets}\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        offset = futures[future]\n",
    "        try:\n",
    "            page_events, page_props = future.result()\n",
    "            all_events.extend(page_events)\n",
    "            print(f\"Fetched page {offset//10000 + 1} with {page_props} properties and {len(page_events)} events\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page at offset {offset}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "ih_2024_df = pd.DataFrame(all_events)\n",
    "\n",
    "# Final verification\n",
    "unique_properties = ih_2024_df['parcl_property_id'].nunique()\n",
    "print(f\"\\nFinal results:\")\n",
    "print(f\"Total events collected: {len(ih_2024_df)}\")\n",
    "print(f\"Unique property IDs: {unique_properties}\")\n",
    "print(f\"Expected property count from API: {total}\")\n",
    "\n",
    "# Display sample data\n",
    "ih_2024_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date and quarter columns\n",
    "ih_2024_df['event_date'] = pd.to_datetime(ih_2024_df['event_date'])\n",
    "ih_2024_df['quarter'] = ih_2024_df['event_date'].dt.to_period('Q')\n",
    "\n",
    "# Calculate all metrics in one go\n",
    "ih_2024_quarterly_metrics = pd.DataFrame({\n",
    "    'acquisition_count': ih_2024_df[ih_2024_df['event_type'] == 'SALE'].groupby('quarter')['parcl_property_id'].nunique(),\n",
    "    'median_rent': ih_2024_df[ih_2024_df['event_type'] == 'RENTAL'].groupby('quarter')['price'].median(),\n",
    "    'rental_listing_count': ih_2024_df[ih_2024_df['event_type'] == 'RENTAL'].groupby('quarter')['parcl_property_id'].nunique()\n",
    "}).reset_index()\n",
    "\n",
    "# Format and display\n",
    "ih_2024_quarterly_metrics['quarter'] = ih_2024_quarterly_metrics['quarter'].astype(str)\n",
    "ih_2024_quarterly_metrics = ih_2024_quarterly_metrics.sort_values('quarter')\n",
    "\n",
    "ih_2024_quarterly_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Leverage the V1 Prop Endpoints for the Quarterly Inventory\n",
    "\n",
    "Inventory is a more complex pull than just point in time metrics, because we need to know if at a given point in time whether or not that event was the latest event for the property. You can pull all events for former or curren IH homes from the V1 endpoints by passing in the csv of parcl prop IDs that have been owned by Invitation Homes at one point in their history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load list of IH Owned Parcl Prop IDs\n",
    "csv_path = '/path_to_you_file.csv'  # Update this to your CSV path\n",
    "property_df = pd.read_csv(csv_path)\n",
    "\n",
    "parcl_property_id_list = property_df['PARCL_PROPERTY_ID'].unique().tolist()\n",
    "\n",
    "print(f\"Loaded {len(parcl_property_id_list)} unique property IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass list of IH Owned Parcl Prop IDs to the V1 Endpoint\n",
    "ih_owned_events = client.property.events.retrieve(\n",
    "        parcl_property_ids=parcl_property_id_list,\n",
    "        end_date='2024-12-31',\n",
    "        event_type='SALE',\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ih_owned_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with all sale events for IH props (current or former) we can backtest inventory by checking, at any point in time if Invitation Homes was the owner on the most recent sale for a property."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
