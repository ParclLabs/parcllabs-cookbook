{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Welcome to the Lab ðŸ¥¼ðŸ§ª</h1>\n",
    "</center>\n",
    "\n",
    "### We will learn how to bulk pull metrics and load as csv files or sql tables for thousands of markets across the country. \n",
    "\n",
    "#### Need help getting started?\n",
    "\n",
    "As a reminder, you can get your Parcl Labs API key [here](https://dashboard.parcllabs.com/signup) to follow along.\n",
    "\n",
    "To run this immediately, you can use Google Colab. Remember, you must set your `PARCL_LABS_API_KEY`.\n",
    "\n",
    "You will need a paid account. \n",
    "\n",
    "Run in collab --> [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ParclLabs/parcllabs-cookbook/blob/main/examples/getting_started/bulk_data_download.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, install and/or upgrade to the latest verison of the Parcl Labs Python library\n",
    "%pip install --upgrade parcllabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from parcllabs import ParclLabsClient\n",
    "\n",
    "client = ParclLabsClient(\n",
    "    api_key=os.environ.get('PARCL_LABS_API_KEY', \"<your Parcl Labs API key if not set as environment variable>\"), \n",
    "    limit=12 # set default limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Market Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all metros's\n",
    "# in this case, lets look at US market overall\n",
    "metros = client.search.markets.retrieve(\n",
    "    sort_by='TOTAL_POPULATION',\n",
    "    sort_order='DESC',\n",
    "    location_type='CBSA',\n",
    "    limit=1000,\n",
    "    auto_paginate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 1000 most populous zipcodes\n",
    "zipcodes = client.search.markets.retrieve(\n",
    "    location_type='ZIP5',\n",
    "    limit=1000,\n",
    "    sort_by='TOTAL_POPULATION',\n",
    "    # auto_paginate=True # if you want to get all zipcodes, set this to true\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare one metadata table for metros and zipcodes\n",
    "# this will allow you to do cross sectional analysis on income, population, etc. \n",
    "market_metadata = pd.concat([metros, zipcodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join zips and metros together to do one pull of all listings\n",
    "parcl_market_ids = metros['parcl_id'].tolist() + zipcodes['parcl_id'].tolist()\n",
    "len(parcl_market_ids) # traversing 1000 most populous zip codes, and 927 metros/micro markets nationwide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Down Data\n",
    "\n",
    "We are going to keep a tight scope, all active inventory and inventory with changing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the analysis start date\n",
    "START_DATE = '2023-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get for sale listings -- weekly metric\n",
    "active_listings = client.for_sale_market_metrics.for_sale_inventory.retrieve(\n",
    "    parcl_ids=parcl_market_ids,\n",
    "    property_type='ALL_PROPERTIES', # can swap this with SINGLE_FAMILY, CONDO or TOWNHOUSE\n",
    "    start_date=START_DATE # once you load into an internal system, will use this to do an incremental pull\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get for sale listings -- weekly metric\n",
    "sfh_active_listings = client.for_sale_market_metrics.for_sale_inventory.retrieve(\n",
    "    parcl_ids=parcl_market_ids,\n",
    "    property_type='SINGLE_FAMILY', \n",
    "    start_date=START_DATE # once you load into an internal system, will use this to do an incremental pull\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get for sale listings -- weekly metric\n",
    "condo_active_listings = client.for_sale_market_metrics.for_sale_inventory.retrieve(\n",
    "    parcl_ids=parcl_market_ids,\n",
    "    property_type='CONDO', # townhouse is another option, would follow the same pattern\n",
    "    start_date=START_DATE # once you load into an internal system, will use this to do an incremental pull\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now have weekly active listings for all metros and zipcodes in one file. \n",
    "# would recommend loading this directly as an augmentation table to your internal system and keeping the market metadata separate. \n",
    "active_listings_by_type = pd.concat([active_listings, sfh_active_listings, condo_active_listings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have one datafile with all active listings, all single family home active listings, and all condo active listings\n",
    "# for every week dating back to 1/1/2023. \n",
    "active_listings_by_type.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's enrich this with price changes to act as a leading indicator for distressed seller signals\n",
    "price_changes = client.for_sale_market_metrics.for_sale_inventory_price_changes.retrieve(\n",
    "    parcl_ids=parcl_market_ids,\n",
    "    property_type='ALL_PROPERTIES', # can swap this with SINGLE_FAMILY, CONDO or TOWNHOUSE\n",
    "    start_date=START_DATE # once you load into an internal system, will use this to do an incremental pull\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfh_price_changes = client.for_sale_market_metrics.for_sale_inventory_price_changes.retrieve(\n",
    "    parcl_ids=parcl_market_ids,\n",
    "    property_type='SINGLE_FAMILY', # can swap this with SINGLE_FAMILY, CONDO or TOWNHOUSE\n",
    "    start_date=START_DATE # once you load into an internal system, will use this to do an incremental pull\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_price_changes = client.for_sale_market_metrics.for_sale_inventory_price_changes.retrieve(\n",
    "    parcl_ids=parcl_market_ids,\n",
    "    property_type='CONDO', # can swap this with SINGLE_FAMILY, CONDO or TOWNHOUSE\n",
    "    start_date=START_DATE # once you load into an internal system, will use this to do an incremental pull\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_price_changes = pd.concat([price_changes, sfh_price_changes, condo_price_changes])\n",
    "all_price_changes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now have three files, one is market metadata, one on inventory for all actives, single family actives, and condos, and another on seller behavior via price changes, days between price changes, etc. We will store these as three separate files, which can all be efficiently joined via the parcl_id index. \n",
    "\n",
    "# to save as flat files, uncomment:\n",
    "# market_metadata.to_csv('market_metadata.csv', index=False)\n",
    "# active_listings_by_type.to_csv('active_listings.csv', index=False)\n",
    "# all_price_changes.to_csv('price_changes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save straight to a database, uncomment and modify the connection string:\n",
    "# import sqlalchemy\n",
    "# engine = sqlalchemy.create_engine('postgresql://user:password@localhost:5432/database')\n",
    "# market_metadata.to_sql('market_metadata', engine, if_exists='replace', index=False)\n",
    "# active_listings_by_type.to_sql('active_listings', engine, if_exists='replace', index=False)\n",
    "# all_price_changes.to_sql('price_changes', engine, if_exists='replace', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parcllabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
