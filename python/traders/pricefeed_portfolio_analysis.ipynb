{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Lab ðŸ¥¼ðŸ§ª\n",
    "\n",
    "Exploratory data analysis of price feeds, portfolio construction, seasonality, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import parcllabs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from prophet import Prophet\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from parcllabs import ParclLabsClient\n",
    "from pypfopt import EfficientFrontier, risk_models, expected_returns\n",
    "\n",
    "api_key = os.getenv('PARCL_LABS_API_KEY')\n",
    "print(f\"Parcl Labs Version: {parcllabs.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Parcl Labs client\n",
    "client = ParclLabsClient(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get all US markets currently available to trade on the Parcl Exchange\n",
    "# Now lets say you want all price feed markets that are on the parcl exchange\n",
    "market_df = client.search_markets.retrieve(\n",
    "    sort_by='PARCL_EXCHANGE_MARKET',\n",
    "    sort_order='DESC',\n",
    "    as_dataframe=True,\n",
    "    params={'limit': 14},  # expand the default limit to 14, as of this writing, 14 markets are available\n",
    ")\n",
    "\n",
    "# manually add Miami City until API updated\n",
    "miami = client.search_markets.retrieve(\n",
    "    query='Miami',\n",
    "    location_type='CITY',\n",
    "    params={'limit': 1},\n",
    "    as_dataframe=True\n",
    ")\n",
    "\n",
    "miami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_df = pd.concat([market_df, miami], axis=0)\n",
    "parcl_ids = market_df['parcl_id'].tolist()\n",
    "market_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets retrieve data back to 2011 for these price feeds\n",
    "START_DATE = '2020-01-01'\n",
    "feeds = client.price_feed.retrieve_many(\n",
    "    parcl_ids=parcl_ids,\n",
    "    start_date=START_DATE,\n",
    "    as_dataframe=True,\n",
    "    params={'limit': 1000},  # expand the limit to 1000, these are daily series\n",
    "    auto_paginate=True, # auto paginate to get all the data - WARNING: ~6k credits can be used in one parcl price feed. Change the START_DATE to a more recent date to reduce the number of credits used\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeds = feeds.merge(market_df[['parcl_id', 'name']], on='parcl_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corr_matrix(\n",
    "        data,\n",
    "        title: str='Correlation Matrix Heatmap',\n",
    "        output_title: str='Median Correlation Coefficient'\n",
    "):\n",
    "    \n",
    "    data_pivot = data.pivot(index='date', columns='name', values='price_feed')\n",
    "    correlation_matrix_filtered = data_pivot.corr()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    sns.heatmap(correlation_matrix_filtered, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    return correlation_matrix_filtered.median().reset_index(name=output_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price feed vs. rental correlation matrix\n",
    "corr1 = build_corr_matrix(feeds)\n",
    "std = feeds.groupby('name')['price_feed'].std().reset_index(name='std_since_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to last year\n",
    "feeds_yr = feeds[feeds['date'] > '2023-05-01']\n",
    "corr2 = build_corr_matrix(feeds_yr, title='Correlation Matrix Heatmap (Since `23)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = feeds.pivot(index='date', columns='name', values='price_feed')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets analyze the series more closely, comparing US PF vs. Miami PF\n",
    "# s = pd.merge(pf[['date', 'price_feed']], r[['date', 'rental_price_feed']], on='date', how='inner')\n",
    "ts_1 = data['United States Of America']\n",
    "ts_2 = data['Miami City']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_time_series(time_series):\n",
    "    \"\"\"Normalize the time series to have a mean of 0.\"\"\"\n",
    "    mean = np.mean(time_series)\n",
    "    normalized_series = time_series - mean\n",
    "    return normalized_series\n",
    "\n",
    "# Example usage\n",
    "time_series_1 = normalize_time_series(ts_1)\n",
    "time_series_2 = normalize_time_series(ts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxdist(x_i, x_j):\n",
    "    \"\"\"Calculate the Chebyshev distance between two vectors.\"\"\"\n",
    "    return np.max(np.abs(x_i - x_j))\n",
    "\n",
    "def phi(m, r, time_series_1, time_series_2, epsilon=1e-10):\n",
    "    \"\"\"Calculate the phi value for given embedding dimension m and tolerance r.\"\"\"\n",
    "    N = len(time_series_1)\n",
    "    X = np.array([time_series_1[i:i + m] for i in range(N - m + 1)])\n",
    "    Y = np.array([time_series_2[i:i + m] for i in range(N - m + 1)])\n",
    "    \n",
    "    C = np.zeros(len(X))\n",
    "    for i in range(len(X)):\n",
    "        C[i] = np.sum([maxdist(X[i], Y[j]) <= r for j in range(len(Y))]) / len(Y)\n",
    "    \n",
    "    C += epsilon  # Add a small value to avoid log(0)\n",
    "    return np.sum(np.log(C)) / (N - m + 1)\n",
    "\n",
    "def cross_apen(time_series_1, time_series_2, m, r, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Calculate the Cross Approximate Entropy between two time series.\n",
    "    \n",
    "    :param time_series_1: First time series (array-like).\n",
    "    :param time_series_2: Second time series (array-like).\n",
    "    :param m: Embedding dimension.\n",
    "    :param r: Tolerance (usually a fraction of the standard deviation of the data).\n",
    "    :param epsilon: Small value to avoid log(0).\n",
    "    :return: Cross-ApEn value.\n",
    "    \"\"\"\n",
    "    r *= np.std(time_series_1)\n",
    "    \n",
    "    return phi(m, r, time_series_1, time_series_2, epsilon) - phi(m + 1, r, time_series_1, time_series_2, epsilon)\n",
    "\n",
    "embedding_dimensions = [30, 60]\n",
    "tolerances = [0.1, 0.2]\n",
    "\n",
    "cross_apen_matrix = np.zeros((len(embedding_dimensions), len(tolerances)))\n",
    "\n",
    "# calc cross-apen for each combo\n",
    "for i, m in enumerate(embedding_dimensions):\n",
    "    for j, r in enumerate(tolerances):\n",
    "        cross_apen_value = cross_apen(time_series_1, time_series_2, m, r)\n",
    "        cross_apen_matrix[i, j] = cross_apen_value\n",
    "\n",
    "\n",
    "# m = 60 # Embedding dimension\n",
    "# r = 0.2  # Tolerance (20% of the standard deviation)\n",
    "\n",
    "# cross_apen_value = cross_apen(time_series_1, time_series_2, m, r)\n",
    "# print(\"Cross-ApEn:\", cross_apen_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cax = ax.matshow(cross_apen_matrix, cmap='viridis')\n",
    "\n",
    "# Add color bar\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xticklabels([''] + [str(r) for r in tolerances])\n",
    "ax.set_yticklabels([''] + [str(m) for m in embedding_dimensions])\n",
    "ax.set_xlabel('Tolerance (r)')\n",
    "ax.set_ylabel('Embedding Dimension (m)')\n",
    "ax.set_title('Cross-ApEn Values')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cross_correlation(ts1, ts2, max_lag):\n",
    "    \"\"\"Calculate cross-correlation between two time series for a range of lags.\"\"\"\n",
    "    lags = np.arange(-max_lag, max_lag + 1)\n",
    "    cross_correlation = []\n",
    "    for lag in lags:\n",
    "        if lag >= 0:\n",
    "            ts1_shifted = ts1[:-lag] if lag != 0 else ts1\n",
    "            ts2_shifted = ts2[lag:]\n",
    "        else:\n",
    "            ts1_shifted = ts1[-lag:]\n",
    "            ts2_shifted = ts2[:lag] if lag != 0 else ts2\n",
    "        \n",
    "        if len(ts1_shifted) > 1 and len(ts2_shifted) > 1:\n",
    "            corr = np.corrcoef(ts1_shifted, ts2_shifted)[0, 1]\n",
    "        else:\n",
    "            corr = 0\n",
    "        cross_correlation.append(corr)\n",
    "    return lags, cross_correlation\n",
    "\n",
    "# Set the maximum lag\n",
    "max_lag = 720\n",
    "\n",
    "# Calculate cross-correlation\n",
    "lags, cross_corr = calculate_cross_correlation(time_series_1, time_series_2, max_lag)\n",
    "\n",
    "# Find the lag with the maximum correlation\n",
    "max_corr_lag = lags[np.argmax(cross_corr)]\n",
    "\n",
    "# Plot the cross-correlation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lags, cross_corr, marker='o')\n",
    "plt.axvline(x=max_corr_lag, color='r', linestyle='--', label=f'Max Correlation Lag: {max_corr_lag}')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Cross-Correlation')\n",
    "plt.title('Cross-Correlation between Time Series (US Housing vs. Miami City)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Interpret the result\n",
    "if max_corr_lag > 0:\n",
    "    print(f\"Time series 1 leads time series 2 by {max_corr_lag} time units.\")\n",
    "elif max_corr_lag < 0:\n",
    "    print(f\"Time series 2 leads time series 1 by {-max_corr_lag} time units.\")\n",
    "else:\n",
    "    print(\"The time series are synchronized with no lag.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt import EfficientFrontier, risk_models, expected_returns\n",
    "\n",
    "data = data.sort_index()\n",
    "returns = data.pct_change().dropna()\n",
    "returns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = expected_returns.mean_historical_return(data, frequency=365)\n",
    "S = risk_models.exp_cov(data, frequency=365)\n",
    "\n",
    "# Optimize for the maximum Sharpe ratio\n",
    "ef = EfficientFrontier(mu, S, weight_bounds=(-1, 1))\n",
    "weights = ef.efficient_return(target_return=0.2, market_neutral=True)\n",
    "# weights = ef.max_sharpe()\n",
    "cleaned_weights = ef.clean_weights()\n",
    "\n",
    "print(\"Optimized Weights:\", cleaned_weights)\n",
    "\n",
    "# Calculate the performance of the optimized portfolio\n",
    "performance = ef.portfolio_performance(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert daily returns to annual returns using 365 days\n",
    "trading_days_per_year = 365\n",
    "annualized_return = (1 + returns.mean())**trading_days_per_year - 1\n",
    "annualized_std = returns.std() * np.sqrt(trading_days_per_year)\n",
    "\n",
    "# Calculate Sharpe Ratio\n",
    "risk_free_rate = 0.01  # Assuming 1% risk-free rate\n",
    "sharpe_ratio = (annualized_return - risk_free_rate) / annualized_std\n",
    "\n",
    "print(\"Annualized Return:\", annualized_return)\n",
    "print(\"Annualized Std Dev:\", annualized_std)\n",
    "print(\"Sharpe Ratio:\", sharpe_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate cumulative returns of the optimized portfolio\n",
    "portfolio_returns = (returns * list(cleaned_weights.values())).sum(axis=1)\n",
    "cumulative_returns = (1 + portfolio_returns).cumprod() - 1\n",
    "\n",
    "# Plot the cumulative returns\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cumulative_returns, label='Optimized Portfolio')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.title('Cumulative Returns of the Optimized Portfolio')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(returns.cumsum().reset_index(), cumulative_returns.reset_index(name='Weighted Portfolio'), on='date', how='inner').plot(x='date', figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_chart(market_name: str, data: pd.DataFrame, pf_type: str = 'price_feed'):\n",
    "\n",
    "    HEIGHT = 900\n",
    "    WIDTH = 1600\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Get a list of up to 15 distinct colors from Plotly\n",
    "    colors = px.colors.qualitative.Plotly\n",
    "\n",
    "    # Add trace for the individual asset cumulative returns\n",
    "    for i, column in enumerate(data.columns):\n",
    "        if column != 'date' and column != 'Weighted Portfolio':\n",
    "            color = colors[i % len(colors)]  # Cycle through the color list\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=data['date'],\n",
    "                y=data[column] * 100,  # Convert to percentage\n",
    "                mode='lines',\n",
    "                line=dict(width=2, color=color),\n",
    "                opacity=0.7,\n",
    "                name=column\n",
    "            ))\n",
    "\n",
    "    # Add the logo image\n",
    "    labs_logo_lookup = {\n",
    "        'blue': 'https://parcllabs-assets.s3.amazonaws.com/powered-by-parcllabs-api.png',\n",
    "        'white': 'https://parcllabs-assets.s3.amazonaws.com/powered-by-parcllabs-api-logo-white+(1).svg'\n",
    "    }\n",
    "    labs_logo_dict = dict(\n",
    "        source=labs_logo_lookup['white'],\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        x=0.5,\n",
    "        y=1.01,\n",
    "        sizex=0.2,\n",
    "        sizey=0.2,\n",
    "        xanchor=\"center\",\n",
    "        yanchor=\"bottom\"\n",
    "    )\n",
    "    fig.add_layout_image(labs_logo_dict)\n",
    "\n",
    "    # Add trace for the weighted portfolio cumulative returns\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=data['date'],\n",
    "        y=data['Weighted Portfolio'] * 100,  # Convert to percentage\n",
    "        mode='lines',\n",
    "        line=dict(width=3, color='red'),\n",
    "        opacity=1.0,\n",
    "        name='Weighted Portfolio'\n",
    "    ))\n",
    "\n",
    "    fig.add_layout_image(\n",
    "        dict(\n",
    "            source=\"path_to_your_logo_image.png\",\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.5, y=1.1,\n",
    "            sizex=0.2, sizey=0.2,\n",
    "            xanchor=\"center\", yanchor=\"top\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=0, r=0, t=110, b=0),\n",
    "        height=HEIGHT,\n",
    "        width=WIDTH,\n",
    "        title={\n",
    "            'text': f'Cumulative Returns: {market_name}',\n",
    "            'y': 0.99,\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top',\n",
    "            'font': dict(size=28, color='#FFFFFF'),\n",
    "        },\n",
    "        plot_bgcolor='#000000',\n",
    "        paper_bgcolor='#000000',\n",
    "        font=dict(color='#FFFFFF'),\n",
    "        xaxis=dict(\n",
    "            title_text='',\n",
    "            showgrid=False,\n",
    "            tickangle=-45,\n",
    "            tickfont=dict(size=14),\n",
    "            linecolor='rgba(255, 255, 255, 0.7)',\n",
    "            linewidth=1\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title_text='Cumulative Returns (%)',\n",
    "            showgrid=True,\n",
    "            gridwidth=0.5,\n",
    "            gridcolor='rgba(255, 255, 255, 0.2)',\n",
    "            tickfont=dict(size=14),\n",
    "            ticksuffix='%',\n",
    "            zeroline=False,\n",
    "            linecolor='rgba(255, 255, 255, 0.7)',\n",
    "            linewidth=1\n",
    "        ),\n",
    "        hovermode='x unified',\n",
    "        hoverlabel=dict(\n",
    "            bgcolor='#1F1F1F',\n",
    "            font_size=14,\n",
    "            font_family=\"Rockwell\"\n",
    "        ),\n",
    "        legend=dict(\n",
    "            x=0,\n",
    "            y=1,\n",
    "            traceorder=\"normal\",\n",
    "            font=dict(\n",
    "                size=12,\n",
    "                color=\"white\"\n",
    "            ),\n",
    "            bgcolor=\"rgba(0,0,0,0)\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    root = f'../../graphics/{pf_type}'\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "    path = os.path.join(root, timestamp)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # Save the plot\n",
    "    fig.write_image(os.path.join(path, f'{market_name}_{pf_type}.png'), width=WIDTH, height=HEIGHT)\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming returns and cumulative_returns are precomputed dataframes\n",
    "merged_data = pd.merge(returns.cumsum().reset_index(), cumulative_returns.reset_index(name='Weighted Portfolio'), on='date', how='inner')\n",
    "build_chart(\"Individual Markets vs. Weighted Portfolio\", merged_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonality analysis\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit Prophet model and extract seasonality\n",
    "def fit_prophet_model(data):\n",
    "    model = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "    model.fit(data)\n",
    "    future = model.make_future_dataframe(periods=0)\n",
    "    forecast = model.predict(future)\n",
    "    return model, forecast\n",
    "\n",
    "# Function to calculate seasonality strength\n",
    "def seasonality_strength(forecast):\n",
    "    seasonal_component = forecast['yearly'].values\n",
    "    total_variance = forecast['yhat'].var()\n",
    "    seasonal_variance = seasonal_component.var()\n",
    "    return seasonal_variance / total_variance\n",
    "\n",
    "# List of dataframes, each containing a time series\n",
    "time_series_list = data.columns.tolist()\n",
    "\n",
    "# Dictionary to store seasonality strengths\n",
    "seasonality_strengths = {}\n",
    "\n",
    "# Process each time series\n",
    "for idx, market_name in enumerate(time_series_list):\n",
    "    model, forecast = fit_prophet_model(data[market_name].reset_index(name='y').rename(columns={'date': 'ds'}))\n",
    "    strength = seasonality_strength(forecast)\n",
    "    seasonality_strengths[f'{market_name}'] = strength\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "seasonality_df = pd.DataFrame.from_dict(seasonality_strengths, orient='index', columns=['Seasonality Strength'])\n",
    "\n",
    "# Rank the time series by seasonality strength\n",
    "seasonality_df = seasonality_df.sort_values(by='Seasonality Strength', ascending=False)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(seasonality_df.index, seasonality_df['Seasonality Strength'], color='skyblue')\n",
    "plt.xlabel('Seasonality Strength')\n",
    "plt.ylabel('Time Series')\n",
    "plt.title('Price Feed Ranking of Time Series by Seasonality Strength')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parcllabs-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
